{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import PorterStemmer\n",
    "STOPS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "\n",
    "## Clean texts\n",
    "def clean_text(text):\n",
    "    \"\"\"Takes in text and returns clean word list\"\"\"\n",
    "    output = re.sub('[^a-zA-Z0-9 ]', '', text).lower()\n",
    "    output = output.split(' ')\n",
    "    output = [o for o in output if o not in STOPS and len(o) > 0]\n",
    "    return ' '.join(output)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "\n",
    "## Replace labels and split into train, test and validation sets\n",
    "def replace_labels(author):\n",
    "    label_dict = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "    return label_dict[author]\n",
    "\n",
    "def replace_labels_vec(author):\n",
    "    label_dict = {'EAP': np.array([1, 0, 0]), 'HPL': np.array([0, 1, 0]), 'MWS': np.array([0, 0, 1])}\n",
    "    return label_dict[author]\n",
    "\n",
    "train_df['author_vector'] = train_df['author'].apply(replace_labels_vec)\n",
    "train_df['author'] = train_df['author'].apply(replace_labels)\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training vocab = 25270\n"
     ]
    }
   ],
   "source": [
    "## Get full training set vocabulary\n",
    "full_train_df = pd.concat([train_df, test_df])\n",
    "word_string = ' '.join(full_train_df['text'].tolist())\n",
    "train_vocab = list(set(word_string.split(' ')))\n",
    "print(\"Size of training vocab = {}\".format(len(train_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split out sentences and form TF-IDF representations of text\n",
    "\n",
    "## Fit TF-DF transformer to training data\n",
    "count_vec = CountVectorizer()\n",
    "X_train_counts = count_vec.fit_transform(train_df['text'].tolist())\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "## Transform test and validation sets\n",
    "X_test_counts = count_vec.transform(test_df['text'].tolist())\n",
    "X_test = tf_transformer.transform(X_test_counts)\n",
    "\n",
    "## Labels\n",
    "y_train = np.array(train_df['author'])\n",
    "y_test = np.array(test_df['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Build a pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score = 0.8251931303070932\n",
      "clf__alpha: 0.08\n",
      "vect__max_df: 0.7\n",
      "vect__min_df: 1\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Grid search for alpha and bigrams/words\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], \n",
    "    'clf__alpha': (0.04, 0.05, 0.06, 0.07,0.08, 0.1),\n",
    "    'vect__max_df': (0.7, 0.8, 0.9, 1.0),\n",
    "    'vect__min_df': (1, 2)\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, verbose=1)\n",
    "gs_clf = gs_clf.fit(train_df['text'].tolist(), np.array(train_df['author']))\n",
    "\n",
    "## Print best score and parameters\n",
    "print(\"Best score = {}\".format(gs_clf.best_score_))\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"{}: {}\".format(param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cross entropy = 0.009267132728268566, train acc = 0.9985315712187959\n",
      "Test cross entropy = 0.14100176501433925, test acc = 0.8465270684371808\n"
     ]
    }
   ],
   "source": [
    "## Evaluating on train and test sets\n",
    "def cross_entropy(preds_mat, labels_mat):\n",
    "    \"\"\"both should by (3, ) vectors\"\"\"\n",
    "    return -np.mean(preds_mat * labels_mat)\n",
    "    \n",
    "## Training set\n",
    "train_preds = gs_clf.predict_log_proba(train_df['text'].tolist())\n",
    "train_labels = np.stack(train_df['author_vector'].tolist())\n",
    "train_Xent = cross_entropy(train_preds, train_labels)\n",
    "train_acc = np.mean(gs_clf.predict(train_df['text'].tolist()) == np.array(train_df['author']))\n",
    "print(\"Train cross entropy = {}, train acc = {}\".format(train_Xent, train_acc))\n",
    "\n",
    "## Test set\n",
    "test_preds = gs_clf.predict_log_proba(test_df['text'].tolist())\n",
    "test_labels = np.stack(test_df['author_vector'].tolist())\n",
    "test_Xent = cross_entropy(test_preds, test_labels)\n",
    "test_acc = np.mean(gs_clf.predict(test_df['text'].tolist()) == np.array(test_df['author']))\n",
    "print(\"Test cross entropy = {}, test acc = {}\".format(test_Xent, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score = 0.8396240870320241\n",
      "clf__alpha: 0.04\n",
      "vect__max_df: 0.7\n",
      "vect__min_df: 1\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Re-train on full training set (train+test)\n",
    "gs_clf_final = gs_clf.fit(full_train_df['text'].tolist(), np.array(full_train_df['author']))\n",
    "\n",
    "## Print best score and parameters\n",
    "print(\"Best score = {}\".format(gs_clf.best_score_))\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"{}: {}\".format(param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in final test set\n",
    "final_test_df = pd.read_csv('data/test.csv')\n",
    "final_test_df['text'] = final_test_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test vocab = 17623\n",
      "Number of words not in training set = 3310\n"
     ]
    }
   ],
   "source": [
    "## Get test set vocabulary\n",
    "word_string = ' '.join(final_test_df['text'].tolist())\n",
    "test_vocab = list(set(word_string.split(' ')))\n",
    "print(\"Size of test vocab = {}\".format(len(test_vocab)))\n",
    "\n",
    "diff = len(test_vocab) - len(set(train_vocab).intersection(set(test_vocab)))\n",
    "print(\"Number of words not in training set = {}\".format(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Load Word2Vec model\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', \n",
    "#                                                         binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Replace words in test set with similar words in training set\n",
    "# train_vocab_set = set(train_vocab)\n",
    "\n",
    "# def get_most_similar(word, vocab=train_vocab_set):\n",
    "#     \"\"\"Gets most similar word2vec word in training set vocab\"\"\"\n",
    "#     if word not in vocab:\n",
    "#         most_similar_word = model.most_similar(['word'], [], 1)[0][0]\n",
    "#         if most_similar_word in vocab:\n",
    "#             return most_similar_word\n",
    "#     return word\n",
    "\n",
    "# def replace_words(text):\n",
    "#     \"\"\"Replaces with similar training set words\"\"\"\n",
    "#     output = text.split(' ')\n",
    "#     output = [get_most_similar(o) for o in output]\n",
    "#     return ' '.join(output)\n",
    "\n",
    "# final_test_df['text'] = final_test_df['text'].apply(replace_words)\n",
    "\n",
    "# ## Check test set vocabulary again\n",
    "# word_string = ' '.join(final_test_df['text'].tolist())\n",
    "# test_vocab = list(set(word_string.split(' ')))\n",
    "# print(\"Size of test vocab = {}\".format(len(test_vocab)))\n",
    "\n",
    "# diff = len(test_vocab) - len(set(train_vocab).intersection(set(test_vocab)))\n",
    "# print(\"Number of words not in training set = {}\".format(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make predictions correct format for submission\n",
    "predictions = gs_clf_final.predict_proba(final_test_df['text'].tolist())\n",
    "final_test_df['EAP'] = predictions[:, 0]\n",
    "final_test_df['HPL'] = predictions[:, 1]\n",
    "final_test_df['MWS'] = predictions[:, 2]\n",
    "final_test_df = final_test_df.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save output to CSV file\n",
    "final_test_df.to_csv('data/third_submission.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
