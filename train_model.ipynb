{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords, words\n",
    "STOPS = stopwords.words(\"english\")\n",
    "\n",
    "MODEL_FOLDER = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Word2Vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8ce86afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "\n",
    "## Clean texts\n",
    "def clean_text(text, model):\n",
    "    \"\"\"Takes in text and returns clean word list\"\"\"\n",
    "    output = re.sub('[^a-zA-Z ]', '', text).lower()\n",
    "    output = output.split(' ')\n",
    "    output = [o for o in output if o not in STOPS and len(o) > 0 and o[:4] != 'http']\n",
    "    output = [o for o in output if o in model.vocab]\n",
    "    return output\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text, args=(model,))\n",
    "\n",
    "## Choose a MAX_LEN\n",
    "text_lengths = [len(t) for t in train_df['text'].tolist()]\n",
    "plt.hist(text_lengths, bins=30)\n",
    "MAX_LEN = 60\n",
    "\n",
    "## Replace labels and split into train, test and validation sets\n",
    "def replace_labels(author):\n",
    "    label_dict = {'EAP': np.array([1, 0, 0]), 'HPL': np.array([0, 1, 0]), 'MWS': np.array([0, 0, 1])}\n",
    "    return label_dict[author]\n",
    "\n",
    "train_df['author'] = train_df['author'].apply(replace_labels)\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>id26911</td>\n",
       "      <td>[acting, natural, live]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12823</th>\n",
       "      <td>id25908</td>\n",
       "      <td>[silent, sun, set, far, hills, blazed, red, go...</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14686</th>\n",
       "      <td>id01435</td>\n",
       "      <td>[illnesses, epoch, alarming, character, alarmi...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6286</th>\n",
       "      <td>id25399</td>\n",
       "      <td>[well, much, harangue, keep, running, instead,...</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18764</th>\n",
       "      <td>id23354</td>\n",
       "      <td>[size, life, perfect, imitation, living, anima...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text     author\n",
       "2585   id26911                            [acting, natural, live]  [1, 0, 0]\n",
       "12823  id25908  [silent, sun, set, far, hills, blazed, red, go...  [0, 1, 0]\n",
       "14686  id01435  [illnesses, epoch, alarming, character, alarmi...  [1, 0, 0]\n",
       "6286   id25399  [well, much, harangue, keep, running, instead,...  [0, 1, 0]\n",
       "18764  id23354  [size, life, perfect, imitation, living, anima...  [1, 0, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape - X: (12530, 60, 300), y: (12530, 3)\n",
      "Validation data shape - X: (3133, 60, 300), y: (3133, 3)\n",
      "Test data shape - X: (3916, 60, 300), y: (3916, 3)\n"
     ]
    }
   ],
   "source": [
    "## Get training matrices\n",
    "def text2mat(clean_text, max_len):\n",
    "    \"\"\"Convert clean list of words from tweet into max_len x E matrix\"\"\"\n",
    "    if len(clean_text) > max_len:\n",
    "        clean_text = clean_text[:max_len]\n",
    "    word_mat = [model[w] for w in clean_text]\n",
    "    padding = [np.zeros(300)] * (max_len - len(word_mat))\n",
    "    word_mat += padding\n",
    "    word_mat = np.stack(word_mat, axis=0)\n",
    "    return word_mat\n",
    "\n",
    "def get_matrices(df):\n",
    "    \"\"\"Pass in test or train and return (?, max_len, E) X and (?) length vector y\"\"\"\n",
    "    mat_list = [text2mat(t, max_len=MAX_LEN) for t in df['text'].tolist()]\n",
    "    X = np.stack(mat_list, axis=0)\n",
    "    y = np.stack(df['author'])\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = get_matrices(train_df)\n",
    "print(\"Training data shape - X: {0}, y: {1}\".format(X_train.shape, y_train.shape))\n",
    "X_val, y_val = get_matrices(val_df)\n",
    "print(\"Validation data shape - X: {0}, y: {1}\".format(X_val.shape, y_val.shape))\n",
    "X_test, y_test = get_matrices(test_df)\n",
    "print(\"Test data shape - X: {0}, y: {1}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(object):\n",
    "    def __init__(self, filter_sizes, num_filters, max_len, tf_model_folder, learning_rate, batch_size):\n",
    "        self.tf_model_folder = tf_model_folder\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_size = 300\n",
    "        self.max_len = max_len\n",
    "        self.num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = 3\n",
    "\n",
    "        # Set up training parameters and TF placeholders\n",
    "        self.x = tf.placeholder(dtype=tf.float32, shape=[None, self.max_len, self.embedding_size], name='x')\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=[None, self.n_classes], name='y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.embeddings_net()\n",
    "        \n",
    "    def embeddings_net(self):\n",
    "        x_expand = tf.expand_dims(self.x, -1)\n",
    "        \n",
    "#         W1 = tf.get_variable(\"W1\", [32, 32],\n",
    "#                              initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "#         W2 = tf.get_variable(\"W2\", [32, 3],\n",
    "#                              initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "#         b1 = tf.get_variable(\"b1\", [32],\n",
    "#                              initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "#         b2 = tf.get_variable(\"b2\", [3],\n",
    "#                              initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "        \n",
    "#         # LSTM layer\n",
    "# #         lstm = tf.contrib.rnn.BasicLSTMCell(num_units=32)\n",
    "#         lstm = tf.nn.rnn_cell.BasicLSTMCell(num_units=32)\n",
    "#         outputs, state = tf.nn.dynamic_rnn(cell=lstm, dtype=tf.float32, inputs=self.x)\n",
    "\n",
    "#         # Linear layer + ReLU + linear layer\n",
    "#         final_output = outputs[:, -1, :]\n",
    "#         h1 = tf.nn.relu(tf.matmul(final_output, W1) + b1)\n",
    "#         x_out = tf.matmul(h1, W2) + b2\n",
    "#         self.y_pred = tf.nn.softmax(x_out)\n",
    "        \n",
    "        \n",
    "        W_out = tf.get_variable(\"W_out\", [self.num_filters_total, 3], \n",
    "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "        b_out = tf.get_variable(\"b_out\", [1], initializer=tf.constant_initializer(0.001))\n",
    "\n",
    "        # Convolutions for s\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.01), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.001, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(tf.cast(x_expand, tf.float32), W, \n",
    "                                    strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, self.max_len-filter_size+1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Fully connected for s\n",
    "        h_pool = tf.concat(pooled_outputs, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, self.num_filters_total])\n",
    "        h_pool_flat = tf.nn.dropout(h_pool_flat, self.keep_prob)\n",
    "        x_out = tf.matmul(h_pool_flat, W_out) + b_out\n",
    "        self.y_pred = tf.nn.softmax(x_out)\n",
    "        \n",
    "        # Compute target and incur loss\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=x_out))\n",
    "#         self.acc = tf.reduce_ps 12sum(tf.cast(tf.equal(tf.round(self.y_pred), self.y), 'float'))\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def save_tf_model(self, tf_session, tf_saver):\n",
    "        return tf_saver.save(tf_session, \"/\".join([self.tf_model_folder, \"tf_model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------- Running for epoch 1 of 20\n",
      "Percent complete: [>                   ] 0%\tVal loss = 1.097347617149353\n",
      "Percent complete: [-->                 ] 13%\tVal loss = 1.033128261566162\n",
      "Percent complete: [---->               ] 26%\tVal loss = 0.9621013402938843\n",
      "Percent complete: [------->            ] 39%\tVal loss = 0.9347442984580994\n",
      "Percent complete: [--------->          ] 51%\tVal loss = 0.9179359674453735\n",
      "Percent complete: [------------>       ] 64%\tVal loss = 0.8863412141799927\n",
      "Percent complete: [-------------->     ] 77%\tVal loss = 0.8802177309989929\n",
      "Percent complete: [----------------->  ] 90%\tVal loss = 0.8772099018096924\n",
      "Percent complete: [------------------->] 100%\n",
      "Trained for 1 epochs: train loss = 0.8809593915939331\n",
      "#------- Running for epoch 2 of 20\n",
      "Percent complete: [>                   ] 0%\tVal loss = 0.8573670387268066\n",
      "Percent complete: [-->                 ] 13%\tVal loss = 0.8513035774230957\n",
      "Percent complete: [---->               ] 26%\tVal loss = 0.8418809771537781\n",
      "Percent complete: [------->            ] 39%\tVal loss = 0.8250436186790466\n",
      "Percent complete: [--------->          ] 51%\tVal loss = 0.8258693814277649\n",
      "Percent complete: [------------>       ] 64%\tVal loss = 0.8206894993782043\n",
      "Percent complete: [-------------->     ] 77%\tVal loss = 0.8232966661453247\n",
      "Percent complete: [----------------->  ] 90%\tVal loss = 0.8356241583824158\n",
      "Percent complete: [------------------->] 100%\n",
      "Trained for 2 epochs: train loss = 0.7791186571121216\n",
      "#------- Running for epoch 3 of 20\n",
      "Percent complete: [>                   ] 0%\tVal loss = 0.8140398859977722\n",
      "Percent complete: [-->                 ] 13%\tVal loss = 0.8302313089370728\n",
      "Percent complete: [---->               ] 26%\tVal loss = 0.7893677353858948\n",
      "Percent complete: [------->            ] 39%\tVal loss = 0.7999618053436279\n",
      "Percent complete: [--------->          ] 51%\tVal loss = 0.7916374206542969\n",
      "Percent complete: [------------>       ] 64%\tVal loss = 0.7927609086036682\n",
      "Percent complete: [-------------->     ] 77%\tVal loss = 0.7985038757324219\n",
      "Percent complete: [----------------->  ] 90%\tVal loss = 0.8064694404602051\n",
      "Percent complete: [------------------->] 100%\n",
      "Trained for 3 epochs: train loss = 0.749258279800415\n",
      "#------- Running for epoch 4 of 20\n",
      "Percent complete: [>                   ] 0%\tVal loss = 0.7888479828834534\n",
      "Percent complete: [-->                 ] 13%\tVal loss = 0.7877801060676575\n",
      "Percent complete: [---->               ] 26%\tVal loss = 0.7841463088989258\n",
      "Percent complete: [------->            ] 39%\tVal loss = 0.7842671871185303\n",
      "Percent complete: [--------->          ] 51%\tVal loss = 0.7757001519203186\n",
      "Percent complete: [------------>       ] 64%\tVal loss = 0.7795592546463013\n",
      "Percent complete: [-------------->     ] 77%\tVal loss = 0.7936420440673828\n",
      "Percent complete: [----------------->  ] 90%\tVal loss = 0.7871471643447876\n",
      "Percent complete: [------------------->] 100%\n",
      "Trained for 4 epochs: train loss = 0.7094212174415588\n",
      "#------- Running for epoch 5 of 20\n",
      "Percent complete: [>                   ] 0%\tVal loss = 0.7848597168922424\n",
      "Percent complete: [>                   ] 7%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-ada552dab428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             }\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_results_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/admin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/admin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/admin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/admin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/admin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Train model\n",
    "# Hyper-parameters\n",
    "filter_sizes = [1, 2, 3, 4]\n",
    "num_filters = 5\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "dropout = 0.55\n",
    "save_results_freq = 50\n",
    "\n",
    "# Create graph\n",
    "tf.reset_default_graph()\n",
    "cnn = CNNClassifier(filter_sizes, num_filters, MAX_LEN, MODEL_FOLDER, learning_rate, batch_size)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Split into batches\n",
    "X_train_batch_list = np.array_split(X_train, int(len(train_df)/batch_size))\n",
    "y_train_batch_list = np.array_split(y_train, int(len(train_df)/batch_size))\n",
    "\n",
    "# To print progress and store results throughout training\n",
    "def progress_bar(value, endvalue, bar_length=20):\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rPercent complete: [{0}] {1}%\".format(arrow + spaces, int(round(percent*100))))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "train_results_dict = {\n",
    "    'n_examples': [], \n",
    "    'batch_train_loss': [], 'validation_loss': []\n",
    "}\n",
    "\n",
    "# Start session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"#------- Running for epoch {} of {}\".format(epoch+1, n_epochs))\n",
    "        \n",
    "        for i in range(len(X_train_batch_list)):\n",
    "            progress_bar(i+1, len(X_train_batch_list))\n",
    "            j = 0\n",
    "            train_dict = {\n",
    "                cnn.x: X_train_batch_list[i], \n",
    "                cnn.y: y_train_batch_list[i], \n",
    "                cnn.keep_prob: 1-dropout\n",
    "            }\n",
    "            opt, loss, pred = sess.run([cnn.opt, cnn.loss, cnn.y_pred], feed_dict=train_dict)\n",
    "            \n",
    "            if i % save_results_freq == 0:\n",
    "                val_feed_dict = {cnn.x: X_val, cnn.y: y_val, cnn.keep_prob:1}\n",
    "                loss_val = sess.run(cnn.loss, feed_dict=val_feed_dict)\n",
    "                print(\"\\tVal loss = {}\".format(loss_val))\n",
    "                \n",
    "                train_results_dict['n_examples'].append(i * batch_size + epoch * len(y_train))\n",
    "                train_results_dict['batch_train_loss'].append(loss)\n",
    "                train_results_dict['validation_loss'].append(loss_val)\n",
    "                \n",
    "        all_train_dict = {cnn.x: X_train_batch_list[i], cnn.y: y_train_batch_list[i], cnn.keep_prob:1}\n",
    "        loss_train = sess.run(cnn.loss, feed_dict=all_train_dict)\n",
    "        print(\"\\nTrained for {} epochs: train loss = {}\".format(epoch+1, loss_train))\n",
    "        \n",
    "    cnn.save_tf_model(sess, saver)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
